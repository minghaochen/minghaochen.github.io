<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on MH&#39;s Blog</title>
    <link>https://minghaochen.github.io/post/</link>
    <description>Recent content in Posts on MH&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>MinghaoChen</copyright>
    <lastBuildDate>Fri, 09 Aug 2019 21:40:19 +0800</lastBuildDate>
    
	<atom:link href="https://minghaochen.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>「机器学习」主成分分析PCA</title>
      <link>https://minghaochen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90pca/</link>
      <pubDate>Fri, 09 Aug 2019 21:40:19 +0800</pubDate>
      
      <guid>https://minghaochen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90pca/</guid>
      <description>&lt;h1 id=&#34;为什么要降维&#34;&gt;为什么要降维&lt;/h1&gt;

&lt;p&gt;要说到降维的目的，主要是用来解决过拟合这一问题的，降维的方式主要有三种&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;直接降维—特征选择&lt;/li&gt;
&lt;li&gt;线性降维—PCA(今日份猪脚)，MDS多维尺度法&lt;/li&gt;
&lt;li&gt;非线性降维—流形学习ISOMAP，LLE(Locally Linear Embedding)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;数据矩阵描述&#34;&gt;数据矩阵描述&lt;/h1&gt;

&lt;p&gt;数据：$X=(x_1,x_2,&amp;hellip;,x_N)^T_{N \times P }, x_i \in \mathbb{R}^P $，为了便于后续推导我们将均值和方差表示为矩阵形式，思路就是&lt;strong&gt;把连加符号改写成矩阵乘积的形式&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;采样均值为：
$$
\bar{x} = \frac{1}{N}\sum_{i=1}^N x_i = \frac{1}{N}(x_1,x_2,&amp;hellip;,x_N)\mathbb{1}_{N} = \frac{1}{N}X^T\mathbb{1}_N
$$
采样方差为
$$
S = \frac{1}{N}\sum_{i=1}^N (x_i-\bar{x}) (x_i-\bar{x})^T = \frac{1}{N}X^THH^TX =  \frac{1}{N}X^THX
$$
其中 $H = I_N - \frac{1}{N}\mathbb{1}_N\mathbb{1}_N^T$ 为中心矩阵，作用可以理解为&lt;strong&gt;去均值&lt;/strong&gt;，这里可以中心均镇的转置等于本身，中心矩阵的平方 $HH^T$ 等于中心矩阵 $H$ 本身。&lt;/p&gt;

&lt;h1 id=&#34;pca的主要思想&#34;&gt;PCA的主要思想&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;一个中心：原始特征空间的重构&lt;/li&gt;
&lt;li&gt;两个基本点： 1. 最大投影方差 2. 最小重构代价&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先将数据进行中心化，即 $x_i - \bar{x}$&lt;/p&gt;

&lt;h2 id=&#34;最大投影方差角度&#34;&gt;最大投影方差角度&lt;/h2&gt;

&lt;p&gt;假设有单位投影向量 $u_1,||u_1||_2 = 1$，向量 $x_i - \bar{x}$ 在 $u_1$ 上的投影为 $(x_i - \bar{x})^Tu_1$ 且均值为0，那么目标函数最大化投影方差可以直接表示为
$$
\max J=\frac{1}{N} \sum_{i=1}^N((x_i - \bar{x})^Tu_1)^2 = u_1^T\cdot S\cdot u_1  \quad s.t. u_1^Tu_1 = 1
$$
这是一个等式约束的最优化问题，直接拉格朗日乘子法写开&lt;/p&gt;

&lt;div&gt;$$
\begin{array}{l} \mathcal{L}\left(u_{1}, \lambda\right)=u_{1}^{T} S u_{1}+\lambda\left(1-u_{1}^{T} u\right)\\ {\frac{\partial \mathcal{L}}{\partial u_{1}}=2 S \cdot u_{1}-\lambda \cdot 2 u_{1}=0}\end{array}
$$&lt;/div&gt;

&lt;p&gt;即
$$
Su_1 = \lambda u_1
$$
转化成&lt;strong&gt;特征值分解&lt;/strong&gt;的问题，所谓的主成分也就是特征向量矩阵，用最大的 $q$ 个特征值对应的特征向量来重构数据矩阵就是特征空间的重构。&lt;/p&gt;

&lt;h2 id=&#34;最小重构代价角度&#34;&gt;最小重构代价角度&lt;/h2&gt;

&lt;p&gt;何谓重构代价？我们先来看下在重构空间中原始数据的表示为
$$
x_i = (x_i^Tu_1)\cdot u_1 + (x_i^Tu_2)\cdot u_2 + &amp;hellip; + (x_i^Tu_p)\cdot u_p = \sum_{k=1}^p (x_i^T\cdot u_k) \cdot u_k
$$
$x_i^Tu_k$ 可以理解成各个投影，$u_k$ 为投影方向。
如果将特征进行压缩，用 $q$ 个特征来表示原始特征空间，则
$$
\hat{x}_i = (x_i^Tu_1)\cdot u_1 + (x_i^Tu_2)\cdot u_2 + &amp;hellip; + (x_i^Tu_q)\cdot u_q = \sum_{k=1}^q (x_i^T\cdot u_k) \cdot u_k
$$
上面两个式子均假设 $x_i$ 中心化过了。那么重构代价很直观的理解就是 $x_i - \hat{x}_i$，目标函数就能表示为
$$
\min J = \sum_{i=1}^N ||x_i - \hat{x}_i ||^2=\sum_{k=q+1}^p u_k^T \cdot S \cdot u_k \quad s.t. u_k^T\cdot u_k =1
$$
由于 $u_k$ 之间是无关的，所以这个优化问题可以拆成单个的优化问题逐一求解。也就转化成&lt;strong&gt;特征值求解&lt;/strong&gt;问题，即求得最小 $p-q$ 个特征值所对应的特征向量。&lt;/p&gt;

&lt;h2 id=&#34;svd角度&#34;&gt;SVD角度&lt;/h2&gt;

&lt;p&gt;前面的两个基本点相当于都是从&lt;strong&gt;方差矩阵 $S$ 进行特征值分解&lt;/strong&gt;来获得主成分的。下面来看看如果直接对数据矩阵进行奇异值分解，两者之间会有什么样的联系。&lt;/p&gt;

&lt;p&gt;对中心化后的数据进行SVD分解：
$$
HX = U \Sigma V^T
$$
原来的方差矩阵可以表示为
$$
S = X^THX=X^TH^THX = V\Sigma U^TU\Sigma V^T=V\Sigma^2V^T
$$
也就是说&lt;strong&gt;对 $HX$ 进行奇异值分解得到的 $V$ 矩阵就是对方差矩阵进行特征值分解得到的特征矩阵，奇异值分解得到到奇异值矩阵的平方就是特征值分解得到的特征值矩阵。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;构造矩阵
$$T =HXX^TH=U\Sigma V^TV\Sigma U^T=U\Sigma^2U^T$$
可以看出 $T$ 和 $S$ 具有&lt;strong&gt;相同的特征值。&lt;/strong&gt;
要获得重构空间的坐标有两个思路：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;内积求投影：$HX\cdot V$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对矩阵 $T$ 进行特征分解：
$$
TU\Sigma = U\Sigma^2U^TU\Sigma= U\Sigma^3 = \Sigma^2\cdot U\Sigma
$$
$$
HX \cdot V=U \Sigma V^T V= U \Sigma
$$&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;因此在遇到原始数据特征空间较高时($P$ 大于 $N$)，可以采用 $T$ 矩阵进行特征分解直接获得坐标，也称为主坐标分解(PCoA)&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;概率主成分分析-p-pca&#34;&gt;概率主成分分析(P-PCA)&lt;/h2&gt;

&lt;p&gt;从概率的角度来看则是将观测数据 $x \in \mathcal{R}^p$ 作为观测变量(observed variable)，重构特征空间 $z \in \mathcal{R}^q$ 作为隐变量(latent variable)，我们降维的过程则相当于从观测变量去求得隐变量的过程
&lt;strong&gt;假设&lt;/strong&gt;&lt;/p&gt;

&lt;div&gt;$$
\begin{equation}
\begin{aligned}
z &amp;\in \mathcal{N}(0,I_q)\\
x &amp;= Wz + \mu + \epsilon\\
\epsilon &amp;\in \mathcal{N}(0,\sigma^2 I_p)
\end{aligned}
\end{equation}
$$&lt;/div&gt;

&lt;p&gt;且 $z$ 和 $\epsilon$ 相互独立。这是一个线性高斯模型，相当于我们有了 $z, x|z, x$ 要求 $z|x$。&lt;/p&gt;

&lt;p&gt;第一步就是Inference求后验 $z|x$ (通过构造 $x,z$ 的联合概率求条件概率)&lt;/p&gt;

&lt;p&gt;第二步就是Learning参数 $W, \mu, \sigma$（比如采用EM算法）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;P-PCA与GMM的区别&lt;/strong&gt;：P-PCA的隐变量是连续的，而GMM的隐变量是离散的。&lt;/p&gt;

&lt;h2 id=&#34;matlab-实现&#34;&gt;Matlab 实现&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;% Data 两个特征线性关系加点噪声
X1 = [1,2,3,4,5,6]&amp;#39;;
X2 = [X1] + rand(6,1);
X = [X1,X2];
plot(X1,X2)
N = size(X,1);

% 矩阵描述
x_bar = 1/N*X&amp;#39;*ones(N,1);
H = eye(N) - 1/N*ones(N,1)*ones(N,1)&amp;#39;;
S = 1/N*X&amp;#39;*H*X;

% 根据采样方差特征值分解
[G,K] = eig(S);

% 根据中心化的数据进行奇异值分解
[U,Sigma,V] = svd(H*X);

% 方差矩阵的特征值与奇异值分解的奇异值的关系
Sigma.^2/N
K
% 观察V矩阵和G矩阵的关系
G
V

% 主坐标分析
T = H*X*X&amp;#39;*H&amp;#39;;
[G2,K2] = eig(T);
% 观察非零特征值对应特征向量也就是主坐标与HXV的关系
H*X*V&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>「机器学习」特征工程</title>
      <link>https://minghaochen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</link>
      <pubDate>Mon, 05 Aug 2019 11:22:25 +0800</pubDate>
      
      <guid>https://minghaochen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</guid>
      <description>&lt;p&gt;&lt;strong&gt;数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;那么我们需要怎么来抬高这一上限呢，特征工程的流程还是比较常规的，这里讲一下主要的思路，只要思路理清了，具体的实现调调包一般就OK啦。（sklearn统统搞定~）&lt;/p&gt;

&lt;p&gt;当你拿到一堆数据时，你的第一反应想干嘛？&lt;/p&gt;

&lt;p&gt;肯定想画几个特征看看它们长什么样吧，那如果不同特征之间规格都不一样，假设你把它们画在同一坐标轴也观察比较不了，而且也会影响后续的训练效率，所以很自然的想法就是先把数据的规格统一一下，那就是所谓的无量纲化；&lt;/p&gt;

&lt;p&gt;那你在无量纲化难免遇到缺失数据，这时候当然是要补全数据啦，最直接的做法就是拿平均值来替代一下；&lt;/p&gt;

&lt;p&gt;还有可能需要做的就是针对不同的算法进行特征的转化，算法中有的需要定性特征有的需要定量特征，转化的思想有点“分类”的思想，比如根据阈值将定量特征“分类”为0或1（大于某个阈值为1否则为0），再比如根据定性数据的值“分类”，有几个值就分几类，相当于拓展几个特征，只有对应特征的值为1，其他特征的值为0，这也叫做哑编码（比如一个特征的值有1、2、3三种可能情况；那就可以拓展为3个特征，原特征值为1时，对应新特征”1“的位置为1，其他位置为0）。&lt;/p&gt;

&lt;p&gt;以及还有按照一定规则进行特征转化的，比如多项式、或者自定义规则。以上这些过程呢也称为&lt;strong&gt;数据预处理&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;当你把手头上的数据处理一波后，起码这些数据能够拿来进行训练了，但是还是要进一步提高所谓的&lt;strong&gt;上限&lt;/strong&gt;呀，你拿到的数据可能特征非常多，含有很多无用特征，自然的想法就是把那些无用的特征剔除掉，选择有用的特征来训练，这就是&lt;strong&gt;特征选择&lt;/strong&gt;的过程，那什么样的数据称为”有用“呢，一般来说两个基本原则吧：发散就是说方差要大，如果一个特征没什么变化，那肯定反应不出目标的变化；相关就很直白了，和目标关系越大的特征当然越有用了。&lt;/p&gt;

&lt;p&gt;当你把有用的特征选择出来后，结果还是发现特征维度还是太高了，这会影响你的训练效率，维度高的解决办法很自然就是降低维度啊，那就轮到降维算法出场了，主要有PCA和LDA，降维的目标相当于用较低维度的数据来表征原始数据的特征，也就是&lt;strong&gt;特征提取&lt;/strong&gt;啦。&lt;/p&gt;

&lt;p&gt;OK了，经过&lt;strong&gt;数据预处理、特征选择、特征提取&lt;/strong&gt;，现在你手上有的就是&lt;strong&gt;较低维度的有用数据&lt;/strong&gt;了吧，快拿去train一发试试！&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://minghaochen.github.io/images/post/%e7%89%b9%e5%be%81%e5%b7%a5%e7%a8%8b/%e7%89%b9%e5%be%81%e5%b7%a5%e7%a8%8b.svg&#34;/&gt; 
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>美音中t的发音</title>
      <link>https://minghaochen.github.io/post/%E7%BE%8E%E9%9F%B3%E4%B8%ADt%E7%9A%84%E5%8F%91%E9%9F%B3/</link>
      <pubDate>Sun, 04 Aug 2019 18:33:34 +0800</pubDate>
      
      <guid>https://minghaochen.github.io/post/%E7%BE%8E%E9%9F%B3%E4%B8%ADt%E7%9A%84%E5%8F%91%E9%9F%B3/</guid>
      <description>&lt;p&gt;今天做高铁回家路上刚好看了Jason的视频，总结一下美音中 t 的发音技巧，同时感谢一下Jason哥。
&lt;figure&gt;
    &lt;img src=&#34;https://minghaochen.github.io/images/post/%e7%be%8e%e9%9f%b3%e4%b8%adt%e7%9a%84%e5%8f%91%e9%9f%b3/%e7%be%8e%e9%9f%b3%e4%b8%adt%e7%9a%84%e5%90%84%e7%a7%8d%e5%8f%91%e9%9f%b3.svg&#34;/&gt; 
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;这里的说法可能跟网上的一些说法不太一致，这里的 held t就是网上说的 stop t，简单理解就是 t 做出口形舌形但不发音，但是需要停顿一下而不是直接省略，达到所谓的 held 住或者 stop 的效果；这里的发音成 fast d 就是网上说的 flap t，弹舌音，简单理解就是舌头快速弹一下发出 d 的音，也就是发音现象中的浊化。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>「机器学习」高斯过程</title>
      <link>https://minghaochen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/</link>
      <pubDate>Fri, 26 Jul 2019 14:38:14 +0800</pubDate>
      
      <guid>https://minghaochen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/</guid>
      <description>&lt;p&gt;网上机器学习的算法铺天盖地，高斯过程却显得不那么起眼，它与其他大多数算法不一样的地方在于，它提供了&lt;strong&gt;不确定性&lt;/strong&gt;的估计，这篇文章争取在不涉及数学推导的情况下给大家一个直观的理解。&lt;/p&gt;

&lt;h1 id=&#34;什么是不确定性&#34;&gt;什么是不确定性？&lt;/h1&gt;

&lt;p&gt;我觉得最能代表不确定性的应该就是概率分布（probability distribution）了，不了解概率分布也没关系，骰子摇过吧，一个六面均匀的骰子，摇到任一数字的概率是多少？答案是显然的: $\frac{1}{6}$，这就是一个离散的概率分布，有六种可能的结果（朝上的面为1、2、3、4、5或6），每种结果的可能性为 $\frac{1}{6}$。进一步延伸一下，离散概率分布的升级版就是连续概率分布，它的可能结果可就不是简单几种情况那么简单了，可以是任意实数，比如人的身高，中国男性的身高统计出来画一下，你就能看到一个中间高两边低的正态分布的样子了。&lt;/p&gt;

&lt;p&gt;那么已知概率分布后，如何采样呢？很简单呀，比如骰子你摇一次就是从这个离散概率分布中采样一次。&lt;/p&gt;

&lt;h1 id=&#34;贝叶斯推断&#34;&gt;贝叶斯推断&lt;/h1&gt;

&lt;p&gt;一提到贝叶斯推断，可能要吓跑一群人，别跑！等我给你介绍完就不怕了！
其实贝叶斯推断就是&lt;strong&gt;根据我们观察到的现象去更新我们脑袋里的认知。&lt;/strong&gt;再具体点就是：我们在事情没发生前对这件事情有所认知（或者叫做先验），通常可以用一个概率分布来表示，然后在得到一些发生的事实后，我们会对这件事情的认知有所该表（就是得到了后验）；而把这些东西联系到一起的就是大名鼎鼎的&lt;strong&gt;贝叶斯定理&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;还是不够形象？那我们来个例子，上面说过离散概率分布了，这次我们来个连续的看看，来看看身高吧，来猜猜小陈有多高（猜对有奖哦，文末二维码随意扫随意打赏哈），你又不认识我没见过我，怎么知道我有多高呢，那直观的猜测就是小陈是个中国人，小陈的身高服从中国人身高的概率分布（假设你能得到这个概率分布咯）
&lt;figure&gt;
    &lt;img src=&#34;https://minghaochen.github.io/images/post/%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b/%e7%94%b7%e6%80%a7%e8%ba%ab%e9%ab%98.jpg&#34;/&gt; 
&lt;/figure&gt;

然后有一天你在博客上看到了小陈和朋友们的合照
&lt;figure&gt;
    &lt;img src=&#34;https://minghaochen.github.io/images/post/%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b/%e5%90%88%e7%85%a7.jpg&#34;/&gt; 
&lt;/figure&gt;

卧槽！小陈有点高啊，在朋友中是最高的，于是你对小陈身高的概率分布的认识改变了
&lt;figure&gt;
    &lt;img src=&#34;https://minghaochen.github.io/images/post/%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b/%e5%90%8e%e9%aa%8c%e8%ba%ab%e9%ab%98.jpg&#34;/&gt; 
&lt;/figure&gt;
&lt;/p&gt;

&lt;h1 id=&#34;什么是高斯过程&#34;&gt;什么是高斯过程？&lt;/h1&gt;

&lt;p&gt;好了，今天的主角登场了，&lt;strong&gt;高斯过程其实就是函数的概率分布&lt;/strong&gt;，既然是概率分布，再用上面贝叶斯的套路我们就也能够通过训练数据来更新它的概率分布。&lt;/p&gt;

&lt;p&gt;我们来理一下思路，函数是什么，最简单的回顾你初中学的 $y=f(x)$ 就是个函数（随便函数形式都行，一次二次正弦等等），把它画一下就是在 XY 平面上的一条线呗；既然是概率分布还是高斯的，那总有均值吧，总有协方差吧，在这里对应就是均值函数和协方差函数。因此高斯过程本质就是&lt;strong&gt;由均值函数和协方差函数决定的一个随机过程。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;比如有一个sin函数，我们有五个点的训练数据
&lt;figure&gt;
    &lt;img src=&#34;https://minghaochen.github.io/images/post/%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b/data.png&#34;/&gt; 
&lt;/figure&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;Xtrain = np.array([-3, -2, -1, 1, 2]).reshape(5,1)
ytrain = np.sin(Xtrain)
pl.plot(Xtrain, ytrain, &amp;#39;bs&amp;#39;, ms=8)&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;没有训练前我们对这个函数没什么认知，那就假设其均值函数为0（大部分时候也都是当做0来使用的），核函数就取SE核函数吧（这个是什么先不管啦，你就认为是个用来生成协方差矩阵的函数，主要我们是要得到它的协方差呀！核函数里一般会有超参数需要估计，为了简单起见，我直接直接给定超参数，不去估计这个参数了），在这样的先验分布下，你得到的函数会是什么样的呢？我们从这个函数的概率分布中采样三个函数出来看看。
&lt;figure&gt;
    &lt;img src=&#34;https://minghaochen.github.io/images/post/%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b/prior.png&#34;/&gt; 
&lt;/figure&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;import numpy as np
import matplotlib.pyplot as pl
%matplotlib inline
# Test data
n = 50
Xtest = np.linspace(-3, 3, n).reshape(-1,1)

# Define the kernel function
def kernel(a, b, param):
    sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T)
    return np.exp(-.5 * (1/param) * sqdist)

param = 0.1
K_ss = kernel(Xtest, Xtest, param)

# Get cholesky decomposition (square root) of the
# covariance matrix
L = np.linalg.cholesky(K_ss + 1e-15*np.eye(n))
# Sample 3 sets of standard normals for our test points,
# multiply them by the square root of the covariance matrix
f_prior = np.dot(L, np.random.normal(size=(n,3)))

# Now let&amp;#39;s plot the 3 sampled functions.
pl.plot(Xtest, f_prior)
pl.axis([-3, 3, -3, 3])
pl.title(&amp;#39;Three samples from the GP prior&amp;#39;)
pl.show()&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;那再根据训练数据训练一发，也就是相当于前面提到的贝叶斯推断，就能得到函数的后验概率分布
&lt;figure&gt;
    &lt;img src=&#34;https://minghaochen.github.io/images/post/%e9%ab%98%e6%96%af%e8%bf%87%e7%a8%8b/posterior.png&#34;/&gt; 
&lt;/figure&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;# Apply the kernel function to our training points
K = kernel(Xtrain, Xtrain, param)
L = np.linalg.cholesky(K + 0.00005*np.eye(len(Xtrain)))

# Compute the mean at our test points.
K_s = kernel(Xtrain, Xtest, param)
Lk = np.linalg.solve(L, K_s)
mu = np.dot(Lk.T, np.linalg.solve(L, ytrain)).reshape((n,))

# Compute the standard deviation so we can plot it
s2 = np.diag(K_ss) - np.sum(Lk**2, axis=0)
stdv = np.sqrt(s2)
# Draw samples from the posterior at our test points.
L = np.linalg.cholesky(K_ss + 1e-6*np.eye(n) - np.dot(Lk.T, Lk))
f_post = mu.reshape(-1,1) + np.dot(L, np.random.normal(size=(n,3)))

pl.plot(Xtrain, ytrain, &amp;#39;bs&amp;#39;, ms=8)
pl.gca().fill_between(Xtest.flat, mu-2*stdv, mu+2*stdv, color=&amp;#34;#dddddd&amp;#34;)
pl.plot(Xtest, mu, &amp;#39;r--&amp;#39;, lw=2)
pl.axis([-3, 3, -3, 3])
pl.title(&amp;#39;GP posterior&amp;#39;)
pl.show()&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;这个回归效果很差，主要因为核函数的超参数没估计，而且训练的数据太少了。感兴趣可以自己去改改代码试试看。详细的推导和理论可以看06年的那本《Gaussian Process for Machine Learning》&lt;/p&gt;

&lt;h2 id=&#34;划重点&#34;&gt;划重点&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;先验 “加&amp;rdquo; 数据得到后验；后验&amp;rdquo;加权平均&amp;rdquo;输出就是预测！&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;最后的最后上两个公式吧&lt;/p&gt;

&lt;p&gt;&lt;div&gt;$$
p(w|y,X) = \frac{p(y|X,w)p(w)}{p(y|X)}
$$&lt;div&gt;&lt;/p&gt;

&lt;p&gt;&lt;div&gt;$$
p\left(f_{*} | {x}_{*}, X, {y}\right)=\int p\left(f_{*} | {x}_{*}, {w}\right) p({w} | X, {y}) d \mathbf{w}
$$&lt;div&gt;
配合上面那句话好好理解消化一下！&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chol分解与SVD分解</title>
      <link>https://minghaochen.github.io/post/chol%E5%88%86%E8%A7%A3%E4%B8%8Esvd%E5%88%86%E8%A7%A3/</link>
      <pubDate>Thu, 25 Jul 2019 15:27:09 +0800</pubDate>
      
      <guid>https://minghaochen.github.io/post/chol%E5%88%86%E8%A7%A3%E4%B8%8Esvd%E5%88%86%E8%A7%A3/</guid>
      <description>&lt;p&gt;今天说说两个整天见却整天忘的矩阵分解，一个是Chol分解，一个是SVD分解&lt;/p&gt;

&lt;h1 id=&#34;chol分解&#34;&gt;Chol分解&lt;/h1&gt;

&lt;p&gt;对于Chol分解，首先要知道它针对的对象是什么，也就是&lt;strong&gt;埃尔米特矩阵（Hermitian matrix）&lt;/strong&gt;，听着有点难懂，其实就是共轭对称矩阵，再简单点说，我们常见的实对称矩阵就是埃尔米特矩阵的特例。&lt;/p&gt;

&lt;p&gt;那么这个Chol分解分解得到的是什么东西？是干嘛用的呢？
我们先来回答第一个问题，Chol分解是将一个矩阵分解成两个矩阵的乘积，即
$$
A = LL^*
$$
其中，$L$ 是一个&lt;strong&gt;下三角&lt;/strong&gt;矩阵，$L^*$ 是 $L$ 的共轭转置，如果实数那就是转置（i.e., $L^T$）&lt;/p&gt;

&lt;p&gt;分解成这个玩意干嘛用的呢？目前了解到的比较常用的两个地方&lt;/p&gt;

&lt;h2 id=&#34;1-解线性方程呗-比如-ax-b&#34;&gt;1. 解线性方程呗，比如 $Ax=b$&lt;/h2&gt;

&lt;p&gt;你可能会说直接目测方程的解就是 $x = A^{-1}b$，还需要什么Chol分解？如果矩阵 $A$ 是病态的（条件数很大），那么问题就来了，我们在用计算机求解这类问题的时候出现舍入误差可能会导致所答非所问。&lt;/p&gt;

&lt;p&gt;如果对矩阵进行Chol分解再来求解方程呢，原方程需要两步求解&lt;/p&gt;

&lt;p&gt;Step1
$$
Ly=b
$$
Step2
$$
L^Tx=y
$$
打开Matlab感受一下？&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;A = [5,7;7,10];
cond(A)
L = chol(A)&amp;#39;;
cond(L)&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;可以看出分解后求解的两个方程都是良态的且只需储存矩阵的下三角部分即可咯。&lt;/p&gt;

&lt;h2 id=&#34;2-求矩阵的行列式&#34;&gt;2. 求矩阵的行列式&lt;/h2&gt;

&lt;p&gt;既然Chol分解把矩阵分解成下三角阵，那么求解原矩阵的行列式自然非常容易了&lt;/p&gt;

&lt;div&gt;$$
\begin{aligned} \operatorname{det} {A} &amp;=\prod_{i=1}^{n} {L}_{i i}^{2} \\ \log \operatorname{det} {A} &amp;=2 \sum_{i=1}^{n} \log {L}_{i i} \end{aligned}
$$&lt;/div&gt;

&lt;p&gt;其中 ${L}_{i i}$ 为矩阵的对角元素。&lt;/p&gt;

&lt;h1 id=&#34;svd分解&#34;&gt;SVD分解&lt;/h1&gt;

&lt;p&gt;要说SVD分解，首先要提到特征根分解，线代学过吧？忘记了没关系，我们再来复习一下。&lt;/p&gt;

&lt;p&gt;首先还是要知道特征值分解的对象是什么，答案是&lt;strong&gt;方阵&lt;/strong&gt;，对于方阵 $A$，有&lt;/p&gt;

&lt;p&gt;$$
Av=\lambda v
$$&lt;/p&gt;

&lt;p&gt;这时，$v$ 就称为特征向量，$\lambda$ 就是与之对应的特征值。&lt;/p&gt;

&lt;p&gt;那么这个特征值分解分解得到的是什么东西？是干嘛用的呢？&lt;/p&gt;

&lt;p&gt;通过特征值分解，可以把矩阵 $A$ 分解为&lt;/p&gt;

&lt;p&gt;$$
A = Q\Sigma Q^{-1}
$$&lt;/p&gt;

&lt;p&gt;其中 $Q$ 就是特征向量组成的矩阵，$\Sigma$ 就是对角线为对应特征值的对角阵。&lt;/p&gt;

&lt;p&gt;这样分解有什么用呢？我们知道矩阵的本质就是线性变换，那么分解后我不就知道&lt;strong&gt;变换方向的主次&lt;/strong&gt;了，因为我们分解得到的 $\Sigma$ 是一个对角阵且特征值从大到小排列，而特征值所对应的特征向量也就是描述矩阵的变化方向，所以 $Q$ 矩阵就是&lt;strong&gt;按照主要变化到次要变化进行排列&lt;/strong&gt;。那么我们就可以根据自己的需要选择特定个数的变化方向来近似原始矩阵变化，也就是&lt;strong&gt;提取原始矩阵中我们期望的主要特征&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;那么这些和今天的猪脚SVD有什么联系呢？（哪有猪脚，肚肚饿）&lt;/p&gt;

&lt;p&gt;关系可大了，你可以粗糙地把SVD也当做是特征值分解，虽然不太准确。上面说到特征值分解针对的是&lt;strong&gt;方阵&lt;/strong&gt;，可是哪来那么多的方阵啊！遇到非方阵时特征值分解不就GG了，这时候SVD出场了。&lt;/p&gt;

&lt;p&gt;一句话概括SVD是适用&lt;strong&gt;任意&lt;/strong&gt;，是&lt;strong&gt;任意&lt;/strong&gt;矩阵的一种分解方法：&lt;/p&gt;

&lt;p&gt;&lt;div&gt;$$
A_{m \times n} = U_{m \times m} \Sigma_{m \times n}V_{n \times n}^T
$$&lt;div&gt;&lt;/p&gt;

&lt;p&gt;那么奇异值和特征值是怎么对应起来的呢？由于矩阵 $A$ 不一定是方阵，我们乘以它的转置就能得到方阵，那么就能特征值分解了。&lt;/p&gt;

&lt;p&gt;$$
(A^TA)v_i = \lambda_iv_i
$$&lt;/p&gt;

&lt;p&gt;这样求得的 $v_i$ 就组成了SVD中的右奇异矩阵 $V$，此外&lt;/p&gt;

&lt;p&gt;$$
\sigma_i=\sqrt{\lambda_i},u_i=Av_i/\sigma_i
$$&lt;/p&gt;

&lt;p&gt;这里的 $\sigma_i$ 就是所谓的奇异值，它在矩阵 $\Sigma$ 中也是从大到小排列的，而且&lt;strong&gt;减小速度非常快！&lt;/strong&gt;，这可是个好消息，说明我们利用&lt;strong&gt;很少的奇异值就能近似描述原始矩阵&lt;/strong&gt;，相当于是&lt;strong&gt;空间压缩&lt;/strong&gt;，比如我们利用 $r$ 个奇异值来近似原始矩阵（$r$ 远小于 $m$ 或 $n$）&lt;/p&gt;

&lt;p&gt;$$
A_{m \times n} = U_{m \times r} \Sigma_{r \times r}V_{r \times n}^T
$$&lt;/p&gt;

&lt;p&gt;所以总的来说SVD就是用来提取重要特征的一个方法。SVD或者特征值分解的具体实现都有现成的函数可以调用，感兴趣可以通过matlab分解一把再观察一下。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;[Q,Sigma1] = eig(A&amp;#39;*A)   % matlab求特征值不一定按从小到大排序的 
[U,Sigma2,V] = svd(A)&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;观察一下 $Q$ 和 $V$ 的关系以及 $Sigma1$ 和 $Sigma2$ 的关系，可以再根据上面给出的公式人工计算一下 $U$ 矩阵，再和matlab的结果进行比较。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TwinCAT3 with Matlab-simulink 配置及使用</title>
      <link>https://minghaochen.github.io/post/twincat3-with-matlab-simulink/</link>
      <pubDate>Tue, 23 Jul 2019 19:35:03 +0800</pubDate>
      
      <guid>https://minghaochen.github.io/post/twincat3-with-matlab-simulink/</guid>
      <description>这几天忙项目的事，需要用到TwinCAT3这个平台，这是倍福（backoff）公司的一个产品，基于PC的控制软件，并且可以用来编译matla</description>
    </item>
    
    <item>
      <title>美音发音技巧——连读</title>
      <link>https://minghaochen.github.io/post/%E7%BE%8E%E9%9F%B3%E5%8F%91%E9%9F%B3%E6%8A%80%E5%B7%A7%E8%BF%9E%E8%AF%BB/</link>
      <pubDate>Mon, 22 Jul 2019 19:58:54 +0800</pubDate>
      
      <guid>https://minghaochen.github.io/post/%E7%BE%8E%E9%9F%B3%E5%8F%91%E9%9F%B3%E6%8A%80%E5%B7%A7%E8%BF%9E%E8%AF%BB/</guid>
      <description>&lt;p&gt;连读主要有三种情况：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;辅元连读&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;辅辅连读&lt;/p&gt;

&lt;p&gt;a. 发音相同，只发后一个音&lt;/p&gt;

&lt;p&gt;b. 发音点相同，(t和d，b和p等)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;元元连读&lt;/p&gt;

&lt;p&gt;a. 第一个元音以 i (包括/oi/,/ai/,/ei/)结尾，后面加上 y 的音：see (y) it&lt;/p&gt;

&lt;p&gt;b.  第一个元音以u (包括/ou/)结尾，后面加上 w 的音：who (w) is&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>如何搭建hugo博客</title>
      <link>https://minghaochen.github.io/post/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAhugo%E5%8D%9A%E5%AE%A2/</link>
      <pubDate>Mon, 22 Jul 2019 09:27:29 +0800</pubDate>
      
      <guid>https://minghaochen.github.io/post/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAhugo%E5%8D%9A%E5%AE%A2/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;安装git&lt;/li&gt;
&lt;li&gt;安装brew&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装Hugo，建立博客&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;hugo new site myblog&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;下载主题&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;cd myblog
git clone https://github.com/olOwOlo/hugo-theme-even themes/even&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;本地启动&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;hugo server -t even --buildDrafts &lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建一个文章&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;hugo new post/new.md&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;部署到github&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;hugo --theme=even --baseUrl=“https://minghaochen.github.io/“ --buildDrafts
cd public/
git init
git add .
git commit -m “first commit”
git remote add origin https://github.com/minghaochen/minghaochen.github.io.git
git push -u origin master&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;本地写完新博客后更新到github上&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;cd public/
git add -A
git commit -m &amp;#34;update&amp;#34;
git push origin master&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
  </channel>
</rss>