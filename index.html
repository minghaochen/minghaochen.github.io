<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>MH&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="MinghaoChen" />






<meta name="generator" content="Hugo 0.55.6 with theme even" />


<link rel="canonical" href="https://minghaochen.github.io/" />
  <link href="https://minghaochen.github.io/index.xml" rel="alternate" type="application/rss+xml" title="MH&#39;s Blog" />
  <link href="https://minghaochen.github.io/index.xml" rel="feed" type="application/rss+xml" title="MH&#39;s Blog" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link href="/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="MH&#39;s Blog" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://minghaochen.github.io/" />

<meta property="og:updated_time" content="2020-02-05T20:39:13&#43;08:00"/>

<meta itemprop="name" content="MH&#39;s Blog">
<meta itemprop="description" content="">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="MH&#39;s Blog"/>
<meta name="twitter:description" content=""/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">MH&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">MH&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <section id="posts" class="posts">
    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA01/">《经济学人》01</a></h1>
    <div class="post-meta">
      <span class="post-time"> 2020-02-05 </span>
      <div class="post-category">
          <a href="/categories/english/"> ’English‘ </a>
          </div>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      <h1 id="title">Title</h1>

<p>Will the Wuhan virus become a pandemic?
Probably. But public health services can help determine how severe it turns out to be</p>

<ul>
<li>Pandemic: a disease that affects people over a very large area or the whole world</li>
</ul>

<h1 id="paragraph-1">Paragraph 1</h1>

<p>Two things explain why a new infectious disease is so alarming. One is that, at first, it spreads exponentially. As tens of cases become hundreds and hundreds become thousands, the mathematics run away with you, conjuring speculation about a health-care collapse, social and economic upheaval and a deadly pandemic. The other is profound uncertainty. Sparse data and conflicting reports mean that scientists can not rule out the worst case—and that lets bad information thrive.</p>

<ul>
<li>Conjure: to make something appear or happen in a way which is not expected</li>
<li>Speculation: when you guess about the possible causes or effects of something without knowing all the facts. 例：There is speculation that the president is ill.</li>
<li>Upheaval: a very big change that often causes problems</li>
</ul>

<h1 id="paragraph-2">Paragraph 2</h1>

<p>So it is with a new coronavirus, known as2019-nCoV, which has struck in China. The number of reported cases grew from 282 on January 20th to almost 7,800 just nine days later. In that time four reported cases outside mainland China have multiplied to 105 in 19 territories. Doubt clouds fundamental properties of the disease, including how it is passed on and what share of infected people die. Amid the uncertainty, a simulation of a coronavirus outbreak by Johns Hopkins University in October, in which 65m people lost their lives, was put about as a prediction. It is not.</p>

<ul>
<li>Strike: 类似于hit</li>
<li>Cloud: 云延伸出来就是“使模糊”</li>
</ul>

<h1 id="paragraph-3">Paragraph 3</h1>

<p>Those are the right questions, though: will the new virus become a global disease? And how deadly will it be? A definite answer is weeks or months away, but public-health authorities have to plan today. The best guess is that the disease has taken hold in China and there is a high risk that it spreads around the world—it may even become a recurrent seasonal infection. It may turn out to be no more lethal than seasonal influenza, but that would still count as serious. In the short term that would hit the world economy and, depending on how the outbreak is handled, it could also have political effects in China.</p>

<ul>
<li>Take hold: to start to have a definite effect</li>
<li>Lethal: causing death, or able to cause death</li>
</ul>

<h1 id="paragraph-4">Paragraph 4</h1>

<p>The outbreak began in December. The repeated mingling of people and animals in China means that viral mutations that infect humans are likely to arise there; and mass migration to cities means that they are likely to spread between people. This virus probably originated in bats and passed through mammals, such as palm civets (果子狸) or ferret badgers (鼬獾), ending up in Wuhan’s wet market, where wild animals were on sale. Symptoms resemble flu, but can include pneumonia, which may be fatal. About 20\% of reported cases are severe, and need hospital care; about 2\% of them have been fatal. As yet, there is no vaccine or antiviral treatment.</p>

<ul>
<li>Mingling: if two feelings, sounds, smells etc mingle, they mix together with each other</li>
<li>Wet market: 指的是传统市场，卖生鲜的这种</li>
<li>Pneumonia: 肺炎</li>
<li>Vaccine: 疫苗</li>
<li>Antiviral: 复合词的结构，抗病毒</li>
</ul>

<h1 id="paragraph-5">Paragraph 5</h1>

<p>The greatest uncertainty is how many cases have gone unrecorded. Primary health care is rudimentary in China and some of the ill either avoided or were turned away from busy hospitals. Many more may have such mild symptoms that they do not realise they have the disease. Modelling by academics in Hong Kong suggests that, as of January 25th, tens of thousands of people have already been infected and that the epidemic will peak in a few months’ time. If so, the virus is more widespread than thought, and hence will be harder to contain within China. But it will also prove less lethal, because the number of deaths should be measured against a much larger base of infections. As with flu, a lot of people could die nonetheless. In 2017-18 a bad flu season saw symptoms in 45m Americans, and 61,000 deaths.</p>

<ul>
<li>Rudimentary: simple and basic</li>
</ul>

<h1 id="paragraph-6">Paragraph 6</h1>

<p>Scientists have started work on vaccines and on treatments to make infections less severe. These are six to 12 months away, so the world must fall back on public-health measures. In China that has led to the biggest quarantine in history, as Wuhan and the rest of Hubei province have been sealed off. The impact of such draconian measures has rippled throughout China. The spring holiday has been extended, keeping schools and businesses closed. The economy is running on the home-delivery of food and goods.</p>

<ul>
<li>Quarantine: a period of time when a person or animal is kept apart from others in case they are carrying a disease</li>
<li>Draconian: very strict and cruel</li>
<li>Ripple: to pass from one person to another like a wave. 例：Panic rippled through Hollywood as the murders were discovered.</li>
</ul>

<h1 id="paragraph-7">Paragraph 7</h1>

<p>Many experts praise China’s efforts. Certainly, its scientists have coped better with the Wuhan virus than they did with SARS in 2003, rapidly detecting it, sequencing its genome (基因测序), licensing diagnostic kits (诊断试剂) and informing international bodies. China’s politicians come off less well. They left alone the cramped markets filled with wild animals that spawned SARS. With the new virus, local officials in Wuhan first played down the science and then, when the disease had taken hold, enacted the draconian quarantine fully eight hours after announcing it, allowing perhaps 1m potentially infectious people to leave the city first.</p>

<ul>
<li>Cramped: not have enough space. 例：The kitchen was small and cramped.</li>
<li>Spawn: to make a series of things happen or start to exist</li>
<li>Enact: to make a proposal into a law. 例：Congress refused to enact the bill.</li>
</ul>

<h1 id="paragraph-8">Paragraph 8</h1>

<p>That may have undermined a measure which is taking a substantial toll. China’s growth in the first quarter could fall to as little as 2\%, from 6\% before the outbreak. As China accounts for almost a fifth of world output, there will probably be a noticeable dent on global growth. Though the economy will bounce back when the virus fades, the reputation of the Communist Party and even of Xi Jinping may be more lastingly affected. The party claims that, armed with science, it is more efficient at governing than democracies. The heavy-handed failure to contain the virus suggests otherwise.</p>

<ul>
<li>Substantial: large in amount or number; considerable</li>
<li>Dent: a reduction in the amount of something</li>
<li>Contain: to stop something from spreading or escaping. 例：Doctors are struggling to contain the epidemic.</li>
</ul>

<h1 id="paragraph-9">Paragraph 9</h1>

<p>Outside China such quarantines are unthinkable. The medical and economic cost will depend on governments slowing the disease’s spread. The way to do this is by isolating cases as soon as they crop up and tracing and quarantining people that victims have been in contact with—indeed, if the disease burns out in China, that might yet stop the pandemic altogether. If, by contrast, that proves inadequate, they could shut schools, discourage travel and urge the cancellation of public events. Buying time in this way has advantages even if it does not completely stop the disease. Health-care systems would have a greater chance to prepare for the onslaught, and to empty beds that are now full of people with seasonal flu.</p>

<ul>
<li>Crop up: if a problem crops up, it happens or appears suddenly and in an unexpected way; arise. 例：if anything crops up, give me a call.</li>
</ul>

<h1 id="paragraph-10">Paragraph 10</h1>

<p>Despite all those efforts the epidemic could still be severe. Some health systems, in Africa and the slums of Asia’s vast cities, will not be able to isolate patients and trace contacts. Much depends on whether people are infectious when their symptoms are mild (or before they show any at all, as some reports suggest), because such people are hard to spot. And also on whether the virus mutates to become more transmissible or lethal.</p>

<ul>
<li>Slum: an area that is in bad condition, where very poor people live</li>
<li>Spot: to notice someone or something, especially when they are difficult to see or recognize. 例：I spotted a police car behind us.</li>
<li>Mutate: a change in genetic structure. 例：Simple organisms like bacteria mutate rapidly.</li>
</ul>

<h1 id="paragraph-11">Paragraph 11</h1>

<p>The world has never responded as rapidly to a disease as it has to 2019-nCoV. Even so, the virus may still do great harm. As humans encroach on new habitats, farm more animals, gather in cities, travel and warm the planet, new diseases will become more common. One estimate puts their cost at $60bn a year. SARS, MERS, Nipah, Zika, Mexican swine flu: the fever from Wuhan is the latest of a bad bunch. It will not be the last.</p>
    </div>
    <div class="read-more">
      <a href="/post/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA01/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/">概率基础「高斯分布」</a></h1>
    <div class="post-meta">
      <span class="post-time"> 2019-08-31 </span>
      <div class="post-category">
          <a href="/categories/%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80/"> ‘数据基础’ </a>
          </div>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      <p>今天补充一些<strong>有关均值方差的公式</strong>和<strong>高斯分布</strong>的一些性质。</p>

<h2 id="some-formulas-of-mean-and-variance">Some Formulas of Mean and Variance</h2>

<p><strong>定理一</strong>：
We consider two random variables $X$ and $Y$</p>

<div>$$
\begin{aligned}
E(X+Y) &= E(X) + E(Y)\\
V(X\pm Y) &= V(X) \pm 2Cov(X,Y)+V(Y)\\
Cov(X,Y)&=E(XY)-E(X)E(Y)
\end{aligned}
$$</div>

<p><strong>定理二</strong>：
When $X$ is indenpendent of $Y$</p>

<div>$$
\begin{aligned}
E(XY) &= E(X)E(Y)\\
V(X \pm Y) &= V(X) + V(Y)\\
Cov(X,Y)&=0
\end{aligned}
$$</div>

<p><strong>定理三</strong>：
For $n$ random variables $X_1,&hellip;,X_n$</p>

<div>$$
\begin{aligned} 
E(\sum_{i}a_iX_i) &= \sum_{i}a_i\mu_i\\
V(\sum_{i}a_iX_i) &= \sum_{i}\sum_{j}a_ia_jCov(X_i,X_j)
\end{aligned}
$$</div>

<p>where $E(X_i)=\mu_i$ and $a_i$ is a constant value. When $X_1,&hellip;,X_n$ are mutually independent, we have the following:</p>

<div>$$
V(\sum_{i}a_iX_i) = \sum_{i}a_i^2V(X_i)
$$</div>

<h2 id="transformation-of-variables">Transformation of Variables</h2>

<p>When a distribution of $X$ is known, we can find a distribution of $Y$ using the transformation of variables, where $Y$ is a function of $X$.</p>

<p><strong>定理四</strong>：
Distribution of $Y = \phi^{(-1)}(X)$: Let $f_x(x)$ be the pdf of $X$ and $X=\phi(Y)$ be a one-to-one transformation, then the pdf of $Y$ is given by</p>

<div>$$
f_y(y) = |\phi'(y)|f_x(\phi(y))
$$</div>

<p>Example: $X\sim N(0,1),Y = \mu + \sigma X$</p>

<p>Since we have</p>

<p>$$
X = \phi(Y) = \frac{Y-\mu}{\sigma},f_x(x)=\frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}x^2)
$$</p>

<p>then $\phi&rsquo;(y)=1/\sigma$</p>

<div>$$
f_y(y) = \frac{1}{\sqrt{2\pi}|\sigma|}exp(-\frac{(y-\mu)^2}{2\sigma^2})
$$</div>

<p>which indicates the normal distribution with mean $\mu$ and variance $\sigma^2$, denoted by $N(\mu,\sigma^2)$.</p>

<p><strong>Multivariate Case</strong></p>

<p>Let $f_x(x_1,&hellip;,x_n)$ be a joint pdf of $(X_1,&hellip;,X_n)$, and a one-to-one transformation from ($X_1,&hellip;,X_n$) to ($Y_1,&hellip;,Y_n$) is given by</p>

<div>$$
\begin{aligned}
X_1 &=\phi_1(Y_1,...,Y_n)\\
&...\\
X_n &=\phi_n(Y_1,...,Y_n)
\end{aligned}
$$</div>

<p>then we obtain a joint pdf of $Y_1,&hellip;,Y_n$</p>

<div>$$
f_y(y_1,...,y_n) = |J|f_x(\phi_1(y_1,...,y_n),...,\phi_n(y_1,...,y_n))
$$</div>

<p>where $J$ is the Jacobian of the transformation.</p>

<div>
    $$
J=\left|\begin{array}{cccc}{\frac{\partial x_{1}}{\partial y_{1}}} & {\frac{\partial x_{1}}{\partial y_{2}}} & {\cdots} & {\frac{\partial x_{1}}{\partial y_{n}}} \\ {\frac{\partial x_{2}}{\partial y_{1}}} & {\frac{\partial x_{2}}{\partial y_{2}}} & {\cdots} & {\frac{\partial x_{2}}{\partial y_{n}}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial x_{n}}{\partial y_{1}}} & {\frac{\partial x_{n}}{\partial y_{2}}} & {\cdots} & {\frac{\partial x_{n}}{\partial y_{n}}}\end{array}\right|
$$
</div>

<h1 id="gaussian-distribution">Gaussian Distribution</h1>

<h2 id="极大似然估计">极大似然估计</h2>

<p>说起高斯分布大家都很熟悉了，假设一个 $p$ 维变量 $x \in R^p$ 满足高斯分布 $N(\mu,\Sigma)$，则其概率密度函数可以表示为</p>

<div>$$
p(x)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))
$$</div>

<p>当有样本数据 $X_{N \times p}=(x_1,&hellip;,x_N)^T$ 时，我们能通过极大似然法估计高斯分布的均值和方差，即</p>

<div>$$
\theta_{MLE}=\arg\max_{\theta}p(X|\theta)
$$</div>

<p>假设 $x_i$ 服从独立同分布 (i.i.d)，则</p>

<div>$$
\log p(X|\theta) = \sum_{i=1}^N \log p(x_i|\theta)
$$</div>

<p>为了便于计算假设 $p=1$ 且真实高斯分布为 $N(\mu,\sigma^2)$，通过极值条件 (令导数为0) 可以得到</p>

<div>$$
\begin{aligned}
\mu_{MLE}&=\frac{1}{N}\sum_{i=1}^Nx_i\\
\sigma^2_{MLE}&=\frac{1}{N}\sum_{i=1}^N(x_i-\mu_{MLE})^2
\end{aligned}
$$</div>

<p>其中,</p>

<p>均值是无偏估计 $E(\mu_{MLE}) = \mu$</p>

<p>方差是有偏估计 $E(\sigma_{MLE}^2)=\frac{N-1}{N}\sigma^2$，也就是说极大似然估计出来的高斯分布的<strong>方差是偏小的</strong>。</p>

<h2 id="从概率密度函数的角度看高斯分布">从概率密度函数的角度看高斯分布</h2>

<p>注意到高斯分布的概率密度函数 $p(x)$ 本质是关于 $x$ 的函数，且和 $x$ 有关的部分为：</p>

<div>
    $$
    \Phi \triangleq (x-\mu)^T\Sigma^{-1}(x-\mu)
    $$
</div>

<p>一般来说 $\Sigma$ 是半正定矩阵，为了便于分析其性质，这里假设其为正定矩阵，对其进行特征值分解：</p>

<div>
    $$
    \Sigma=U\Lambda U^T=\sum_{i=1}
^pu_i\lambda_iu_i^T $$
</div>

<p>其中，$U=(u_1,&hellip;,u_p),UU^T=U^TU=I,\Lambda=diag(\lambda_i)$</p>

<p>则方差矩阵的逆为</p>

<div>$$
\Sigma^{-1}=(U\Lambda U^T)^{-1}=U\Lambda^{-1}U^T=\sum_{i=1}^pu_i\frac{1}{\lambda_i}u_i^T
$$</div>

<p>定义 $y_i=(x-\mu)^Tu_i$，可以将 $y_i$ 看作是 $x$ 去均值后在向量 $u_i$ 上的<strong>投影</strong>，则 $\Phi$ 可以表示为</p>

<div>$$
\Phi = (x-\mu)^T\Sigma^{-1}(x-\mu)=(x-\mu)^T\sum_{i=1}^pu_i\frac{1}{\lambda_i}u_i^T(x-\mu)=\sum_{i=1}^p\frac{y_i^2}{\lambda_i}
$$</div>

<p>为了便于展示我们取 $p=2$，并令 $\Phi=1$ 可以发现</p>

<div>$$
\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2} = 1
$$</div>

<p><strong>竟然是一个椭圆！</strong></p>

<figure>
    <img src="/images/post/%e3%80%8c%e6%a6%82%e7%8e%87%e5%9f%ba%e7%a1%80%e3%80%8d%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83/Gaussian1.jpg"/> 
</figure>


<p>也就是说指定了 $\Phi$ 的值，相当于能够得到高斯分布的等高线。</p>

<p>Matlab code</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span></pre></td>
<td class="lntd">
<pre class="chroma"> clear;
 clear all;
 clf

 mu = [1,2];
 Sigma = [1,0.5;0.5,2];
 X = mvnrnd(mu,Sigma,500); % 从高斯分布中生成样本
 scatter(X(:,1),X(:,2))
 [U,Lambda] = eig(Sigma)；
 u1 = U(:,1); % 对应博客中的投影向量
 u2 = U(:,2);
 lambda1 = Lambda(1,1); % 对应博客中的椭圆长短轴
 lambda2 = Lambda(2,2);

 X1 = [];
 X2 = [];
 % 采用暴力搜索来获取使得\Phi = 1的横纵坐标
 for x1 = -3:.01:5
     for x2 = -4:.01:5
         phi = (([x1,x2] - mu)*u1)^2/lambda1 + (([x1,x2] - mu)*u2)^2/lambda2;
         if phi &lt;= 1.01 &amp;&amp; phi &gt;= 0.99
             X1 = [X1;x1];
             X2 = [X2;x2];
         end
     end
 end
             
 hold on 
 scatter(X1,X2)

 X1 = [];
 X2 = [];
 % 采用暴力搜索来获取使得\Phi = 2的横纵坐标
 for x1 = -3:.01:5
     for x2 = -4:.01:5
         phi = (([x1,x2] - mu)*u1)^2/lambda1 + (([x1,x2] - mu)*u2)^2/lambda2;
         if phi &lt;= 2.01 &amp;&amp; phi &gt;= 1.99
             X1 = [X1;x1];
             X2 = [X2;x2];
         end
     end
 end
             
 hold on 
 scatter(X1,X2)

 % 画出投影向量
 x = 1:2:3;
 k1 = u1(2)/u1(1);
 k2 = u2(2)/u2(1);
 y1 = k1*(x-mu(1))+mu(2);
 y2 = k2*(x-mu(1))+mu(2);
 plot(x&#39;,y1&#39;,&#39;LineWidth&#39;,2)
 plot(x&#39;,y2&#39;,&#39;LineWidth&#39;,2)
 xlim([-2, 5]);
 ylim([-2, 5]);
 axis square
 legend(&#39;data&#39;,&#39;\Phi=1&#39;,&#39;\Phi=2&#39;,&#39;u1&#39;,&#39;u2&#39;)
 xlabel(&#39;x1&#39;)
 ylabel(&#39;x2&#39;)</pre></td></tr></table>
</div>
</div>
<h2 id="高斯分布的局限性">高斯分布的局限性</h2>

<ol>
<li><p>协方差矩阵 $\Sigma$ 中的参数个数太多 $p(p+1)/2 = O(p^2)$；可以采用<strong>对角化</strong>或<strong>各向同性</strong>的假设。</p></li>

<li><p>单高斯分布来拟合数据不合理；可以采用<strong>混合高斯模型</strong>。</p></li>
</ol>

<h2 id="已知联合概率求边缘概率和条件概率">已知联合概率求边缘概率和条件概率</h2>

<p>已知</p>

<div>$$
x=\left(\begin{array}{l}{x_{a}} \\ {x_{b}}\end{array}\right),
\mu=\left(\begin{array}{l}{\mu_{a}} \\ {\mu_{b}}\end{array}\right),\Sigma=\left(\begin{array}{ll}{\Sigma_{a a}} & {\Sigma_{a b}} \\ {\Sigma_{b a}} & {\Sigma_{b b}}\end{array}\right)
$$</div>

<p>求 $p(x_a),p(x_b|x_a)$。可以采用配方法(见PRML，过于复杂)，这里采用构造定义法。</p>

<p>定义 $A = (I_m \quad 0)$，则 $x_a = Ax$</p>

<div>$$
\begin{aligned}
E[x_a]&=AE[x]=\mu_a\\
Var[x_a]&=A\Sigma A^T=\Sigma_{aa}
\end{aligned}
$$</div>

<p>所以<strong>边缘概率分布为</strong></p>

<div>$$x_a \sim N(\mu_a,\Sigma_{aa})$$</div>

<p>定义</p>

<div>$$x_{b.a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a$$</div>

<div>$$A=(-\Sigma_{ba}\Sigma_{aa}^{-1} \quad I)$$</div>

<p>则 $x_{b.a}=Ax$</p>

<div>$$
    \begin{aligned}
E[x_{b.a}]&=AE[x]=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\
Var[x_{b.a}]&=A\Sigma A^T = \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
    \end{aligned}
$$</div>

<p>则可以得到 $x_{b.a}$ 的分布，又因为</p>

<div>$$x_b=x_{b.a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a$$</div>

<p>条件分布的均值和方差可以表示为</p>

<div>$$
\begin{aligned}
E[x_b|x_a] &= E[x_{b.a}] + \Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
Var[x_b|x_a] &= Var[x_{b.a}] 
\end{aligned}
$$</div>

<p>因此<strong>条件概率分布</strong>为</p>

<div>$$
x_b|x_a \sim N(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a),\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab})
$$</div>

<h2 id="已知边缘概率和条件概率求联合概率">已知边缘概率和条件概率求联合概率</h2>

<p>已知</p>

<div>$$
\begin{aligned}
p(x)&=N(x|\mu,\Lambda^{-1})\\
p(y|x)&=N(y|Ax+b,L^{-1})
\end{aligned}
$$</div>

<p>求 $p(y),p(x|y)$</p>

<p>定义 $y=Ax+b+\epsilon,\epsilon \sim N(0,L^{-1})$</p>

<p>则 $y$ 的边缘概率为</p>

<div>$$
\begin{aligned}
E[y]&=E[Ax+b] + E[\epsilon] = A\mu+b\\
Var[y]&=Var[Ax+b]+Var[\epsilon]=A\Lambda^{-1}A^T+L^{-1}
\end{aligned}
$$</div>

<p>即</p>

<div>$$
y \sim N(A \mu +b, A\Lambda^{-1}A^T+L^{-1})
$$</div>

<p>要求 $p(x|y)$ 可以构造联合分布，在利用联合概率求条件概率</p>

<p>构造</p>

<div>$$
Z=\left(\begin{array}{l}{x} \\ {y}\end{array}\right) \sim \mathcal{N}\left(\left[\begin{array}{c}{\mu} \\ {A \mu+b}\end{array}\right],\left[\begin{array}{cc}{\Lambda^{-1}} & {\Delta} \\ {\Delta} & {L^{-1}+A \Lambda^{-1} A^{T}}\end{array}\right]\right)
$$</div>

<p>也就是只要求出 $x$ 和 $y$ 之间的协方差 $\Delta$ 就能够知道它们的联合分布了。根据协方差的定义来求解</p>

<div>$$
\begin{aligned}
\Delta &= Cov(x,y)\\
&=E[(x-E[x])(y-E[y])^T]\\
&=E[(x-\mu)(Ax+b+\epsilon-(A\mu+b))^T]\\
&=E[(x-\mu)(Ax-A\mu+\epsilon)^T]\\
&=E[(x-\mu)(Ax-A\mu)^T+(x-\mu)\epsilon^T]\\
&=E[(x-\mu)(x-\mu)^T]A^T+E[(x-\mu)]E[\epsilon]\\
&=Var[x]A^T+0\\
&=\Lambda^{-1}A^T
\end{aligned}
$$</div>

<p>这样完整的联合分布就得到了，代入上一节 $x_b|x_a$ 的公式即可得到 $p(x|y)$ 的概率分布了。</p>
    </div>
    <div class="read-more">
      <a href="/post/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    <article class="post">
  <header class="post-header">
    <h1 class="post-title"><a class="post-link" href="/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/">「机器学习」变分推断</a></h1>
    <div class="post-meta">
      <span class="post-time"> 2019-08-28 </span>
      <div class="post-category">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> 机器学习 </a>
          </div>
    </div>
  </header>
  
  <div class="post-content">
    <div class="post-summary">
      <h2 id="前言">前言</h2>

<p>之前一直对“变分推断”这个词有一种恐惧的心理，听着就有点难懂，加上没学过泛函分析这块内容，潜意识里也很抵触&rdquo;变分&rdquo;，导致之前经常有选择性地忽略这一块知识点。如果你也一样和我有这样的心理，那么首先恭喜你称为了我的有缘人，其次再恭喜你看到了这篇博客，我尽可能从外行的角度通俗易懂地把&rdquo;变分推断&rdquo;展示出来，希望对有缘人有所启发。</p>

<p>我还是从<strong>为什么、是什么、怎么做</strong>三方面进行阐述。</p>

<h2 id="为什么要用变分推断">为什么要用变分推断</h2>

<p>一个算法的出现总是有问题驱动的，了解算法要解决的问题能帮助我们更好了解算法的用途。我们知道从贝叶斯角度来看待机器学习问题可以分为两个步骤：<strong>推断</strong>和<strong>决策</strong>。</p>

<p>说的通俗点<strong>推测</strong>就是先求参数的后验</p>

<p>$$p(\theta|X)$$</p>

<p><strong>决策</strong>就是根据后验对测试数据进行预测</p>

<div>$$
p(x_{new}|X) = \int_{\theta} p(x_{new}|\theta) p (\theta|X) d \theta = E_{\theta|X}[p(x_{new}|\theta)]
$$</div>

<p>也就相当于测试数据关于后验的期望。</p>

<p>但问题就在于这个后验的计算通常是非常困难的，对于不能<strong>精确推断</strong> (直接计算)的后验分布，很朴素的想法就是<strong>近似推断</strong>，比如今天要讲的变分推断就属于<strong>确定性近似</strong>的一种方法；以及接下来还会再讲的<strong>随机近似</strong>，比如 MCMC。</p>

<p>第一个问题也就得到回答了，为什么要用变分推断？<strong>因为要计算后验分布的近似分布</strong>。</p>

<h2 id="变分推断是什么">变分推断是什么</h2>

<p>知道了为什么要用变分推断后，其实变分推断做的是什么也就顺理成章了，很朴素的想法就是用简单好搞的分布来逼近难求的后验分布。那怎么评价两个分布有多接近呢？那就看看这两个分布的 KL 散度 (这是非负的)，当 KL 散度为 0 时代表两个分布是一样的，因此变分推断要做的就是优化一个分布 $q(Z)$ 使得其与后验分布 $p(Z|X)$ 的 KL 散度最小化 (其中 $X$ 是观测数据，$Z$ 包含隐变量和参数)，用优化命题的形式来描述就是</p>

<div>$$
q^{*}(Z) = \arg \min_{q(Z)} \text{KL}(q(Z)||p(Z|X))
$$</div>

<p>但是 $p(Z|X)$ 本身就不知道的，这种直接求解的方式显然是行不通的，因此需要找个间接求解的方法。</p>

<p>根据贝叶斯公式有</p>

<div>$$
p(X) = \frac{p(X,Z)}{p(Z|X)} 
$$</div>

<p>两边取对数</p>

<div>
    $$
\ln p(X) = ln (p(X,Z)) - ln (p(Z|X))
    $$
</div>

<p>在推导 EM 算法时我们也是这么做的，先对上式右边加减一个关于 $Z$ 的分布 $q(Z)$，然后左边两边同时关于 $q(Z)$ 求期望。左边求期望还是等于本身 $\ln p(X)$，因此</p>

<div>$$
    \begin{aligned}
\ln p(X) &= \int_Z q(Z)\ln (\frac{p(X,Z)}{q(Z)}） dZ - \int_Zq(Z) \ln(\frac{p(Z|X)}{q(Z)}) dZ\\
&=\text{ELBO} + \text{KL}(q(Z)||p(Z|X))
\end{aligned}
$$</div>

<p>为什么叫 ELBO？通常 $p(X)$ 称为 evidence，而 KL 散度是非负的，所以有</p>

<div>$$
\ln p(X) \ge \int_Z q(Z)\ln (\frac{p(X,Z)}{q(Z)}） dZ 
$$</div>

<p>因此不等式右边这项也就称为 evidence 的 lower bound (ELBO) 了。</p>

<p>我们之前想求的 KL 散度又出现了！虽然搞不定它，但是我们可以搞旁边的 ELBO 啊，刚才最小化 KL 散度不就转化为最大化 ELBO 了！再用优化命题的形式描述就是</p>

<div>$$
q(Z) = \arg \max_{q(Z)} \int_Z q(Z)\ln (p(X,Z)) dZ - \int_Z q(Z)\ln (q(Z)）dZ
$$</div>

<p>因为 $q(Z)$ 是前面假设出来的关于 $Z$ 的分布，对这个分布还没有做任何假设，为了进一步推导便于求解，这里引入<strong>平均场理论</strong>，将 $q(Z)$ 划分为 $M$ 个相互独立的部分，即</p>

<div>$$
q(Z) = \prod_{i=1}^M q_i(Z_i)
$$</div>

<p>采用如下的符号定义</p>

<div>$$
Z = \{Z_j,\bar{Z_j}\},\bar{Z_j} = Z \backslash Z_j
$$</div>

<p>然后代入 ELBO 进行化简，我们一项一项来看</p>

<p>第一项为：</p>

<div>$$
    \begin{aligned}
    &\int_Z q(Z)\ln (p(X,Z)) dZ\\
=&\int_{Z_1}...\int_{Z_M} \prod_{i=1}^M q_i(Z_i) \ln (p(X,Z)) dZ_1...dZ_M\\
=&\int_{Z_j}q_j(Z_j)[\int_{\bar{Z_j}}\prod_{i \not=j}^M q_i(Z_i) \ln(p(X,Z)) d \bar{Z_j}] d Z_j\\
=&\int_{Z_j}q_j(Z_j)E_{q(\bar{Z_j})} [\ln (p(X,Z))] d Z_j
    \end{aligned}
$$</div>

<p>第二项为：</p>

<div>$$
\begin{aligned}
&\int_Z q(Z)\ln (q(Z)）dZ\\
=&\int_{Z_1}...\int_{Z_M} \prod_{i=1}^M q_i(Z_i) \sum_{i=1}^M \ln (q_i(Z_i)) dZ_1...dZ_M\\
=&\sum_{i=1}^M\int_{Z_i} q_i(Z_i)\ln(q_i(Z_i))dZ_i\\
=&\int_{Z_j} q_j(Z_j)\ln(q_j(Z_j))dZ_j + C
\end{aligned}
$$</div>

<p>为了统一描述，定义</p>

<div>$$
    \tilde{p}(X,Z_j) = E_{q(\bar{Z_j})} [\ln (p(X,Z))]
$$</div>

<p>因此 ELBO 可以表示为</p>

<div>$$
    \begin{aligned}
\text{ELBO} &= \int_{Z_j}q_j(Z_j)E_{q(\bar{Z_j})} [\ln (p(X,Z))] d Z_j - \int_{Z_j} q_j(Z_j)\ln(q_j(Z_j))dZ_j - C\\
&=\int_{Z_j} q_j(Z_j)\ln\frac{\tilde{p}(X,Z_j)}{q_j(Z_j)}dZ_j - C\\
&=-\text{KL}(q_j(Z_j)||\tilde{p}(X,Z_j)) - C
\end{aligned}
$$</div>

<p>也就是将 ELBO 转化成另外一个 KL 散度，ELBO取最大值时对应 KL 散度取等号，即</p>

<div>$$
q_j^{*}(Z_j) = \tilde{p}(X,Z_j)
$$</div>

<p>将 $\tilde{p}(X,Z_j)$ 的定义代入得</p>

<div>$$
\ln (q_j^{*}(Z_j)) = E_{q(\bar{Z_j})} [\ln (p(X,Z))]
$$</div>

<p>通俗来说就是用对数联合概率分布关于<strong>利用除了 $Z_j$ 以外的其他分布</strong>的期望来更新 $Z_j$ 的分布。这是一个迭代的过程：</p>

<div>$$
\begin{aligned}
\ln(q_1^{*}(Z_1)) &= \int_{Z_2} \int_{Z_3}... \int_{Z_M} q_2({Z_2})...q_M(z_M)\ln (p(X,Z)) dZ_2d Z_3 ... dZ_M\\
\ln(q_2^{*}(Z_2)) &= \int_{Z_1^*} \int_{Z_3}... \int_{Z_M} q_1^*({Z_1}) q_2({Z_3})...q_M(z_M) \ln (p(X,Z)) dZ_1^* dZ_3 ... dZ_M\\
&...\\
\ln(q_M^{*}(Z_M)) &= \int_{Z_1^*} \int_{Z_2^*}... \int_{Z_{M-1}} q_1^*({Z_1})...q_{M-1}^*(z_{M-1}) \ln (p(X,Z)) dZ_1^* dZ_2^* ... dZ_{M-1}^*
\end{aligned}
$$</div>

<p>迭代收敛得到的分布 $q(Z)$ 就是要求的后验分布的 $p(Z|X)$ 的近似分布了。</p>

<h2 id="变分推断怎么用">变分推断怎么用</h2>

<p>知道了变分推断是怎么一回事后，更多人想要知道具体使用时是怎么用的，这里用一个一元高斯分布的例子进行说明，该例子选自《徐亦达机器学习》系列。结合 matlab 代码跟我一起来实现一下变分推断吧</p>

<p>首先假设你有个数据集 $D =$ {$x_1,&hellip;,x_N$} 来自一个高斯分布 $\mathcal{N}(0,1)$，这个均值和方差我们是不知道的，也就是等下要从数据中去学习的 $p(\mu,\tau|D)$，其中 $\tau=1/\sigma^2$。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></pre></td>
<td class="lntd">
<pre class="chroma">N = 100;
mu_0 = 0; 
sigma_0 = 1;
D = mu_0 + sigma_0* randn(N,1);</pre></td></tr></table>
</div>
</div>
<p>根据贝叶斯公式我们有</p>

<div>$$
p(\mu,\tau|D) \propto p(D|\mu,\tau)p(\mu|\tau)p(\tau)
$$</div>

<p>因此要学习参数的分布，需要先对参数假定一个先验，因为 $\tau$ 大于 0 的性质，假设其为 Gamma 分布 $Gamma(\tau|a_0,b_0)$，$p(\mu|\tau)$ 为高斯分布 $\mathcal{N}(\mu_0,(\lambda_0 \tau)^{-1})$，由于共轭性可以直接得到后验分布的解析形式</p>

<div>$$
p(\mu,\tau|D) = \mathcal{N}(\mu_n,(\lambda_n\tau)^{-1})Gamma(\tau|a_n,b_n)
$$</div>

<p>且参数 $\mu_n,\lambda_n,a_n,b_n$ 直接可以得到</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></pre></td>
<td class="lntd">
<pre class="chroma">% 先验分布里的参数
lambda_0 = 1;
a_0 = 1.5;
b_0 = 1;

% 后验分布的参数 
mu_n = (lambda_0 * mu_0 + N * mean(D) )/ (lambda_0 + N);
lambda_n = lambda_0 + N;
a_n = a_0 + N/2;
b_n = b_0 + 1/2 * sum((X - mean(X)).^2) + (lambda_0*n * (mean(X) - mu_0)^2)/(2*(lambda_0 + N));</pre></td></tr></table>
</div>
</div>
<p>但是我们要假装不知道这个共轭分布能直接这么得到，用变分推断来求求看，首先假设 $q(Z)$ 由独立的两部分组成，在这里就是</p>

<div>$$
q(\mu,\tau) = q_{\mu}(\mu)q_{\tau}(\tau)
$$</div>

<p>根据之前推导得到的变分推断的迭代公式对应为</p>

<div>$$
\begin{aligned}
\ln(q_{\mu}^{*}(\mu)) &= \int_{\tau} q_{\tau}(\tau) \ln (p(X,Z)) d \tau\\
\ln(q_{\tau}^{*}(\tau)) &= \int_{\mu^*}  q_{\mu}^*(\mu) \ln (p(X,Z)) d \mu^*
\end{aligned}
$$</div>

<p>化简为</p>

<div>$$
\begin{aligned} \ln \left(q_{\mu}^{*}(\mu)\right) &=-\frac{E_{q_{\tau}}[\tau]}{2}\left[\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}+\lambda_{0}\left(\mu-\mu_{0}\right)^{2}\right]+\text { const } \\ &=-\frac{E_{q_{\tau}}[\tau]\left(n+\lambda_{0}\right)}{2}\left(\mu-\frac{\left(N \overline{x}+\lambda_{0} \mu_{0}\right)}{\left(N+\lambda_{0}\right)}\right)^{2}+\text { const } \\ &=\mathcal{N}\left(\frac{N \overline{x}+\lambda_{0} \mu_{0}}{N+\lambda_{0}}, E_{q_{\tau}}[\tau]\left(N+\lambda_{0}\right)\right) \end{aligned}
$$</div>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></pre></td>
<td class="lntd">
<pre class="chroma">E_tau = a_current/b_current; % Gamma分布的均值
mu_current       = (lambda_0 * mu_0 + N * mean(X))/(lambda_0 + N);
lambda_current   = (lambda_0 + N) * E_tau;</pre></td></tr></table>
</div>
</div>
<div>
$$
\ln {q^*_{\tau}(\tau)}=(\underbrace{\frac{N}{2}+a_{0}-1}_{a_{n}}) \ln (\tau)-\tau(\underbrace{b_{0}+\frac{1}{2} E_{q_{\mu}^*(\mu)}\left[\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}+\lambda_{0}\left(\mu-\mu_{0}\right)^{2}\right]}_{b_{n}})+\text { const }
$$
</div>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></pre></td>
<td class="lntd">
<pre class="chroma">a_current  = a_0 + N/2;
% E[mu^2] = var(mu) + (E[mu])^2
E_mu_square = inv(lambda_0) + mu_prev^2;
% E[mu]
E_mu = mu_prev;
%sum [(x_i - mu)^2]
first = sum( X.^2 - 2 * X .* repmat(E_mu,size(X)) + repmat(E_mu_square, size(X)));
%lambda_0 (mu - mu_0)^2
second = lambda_0 *(E_mu_square - 2*mu_0*E_mu + mu_0^2);
b_current = b_0 + (first + second)/2;</pre></td></tr></table>
</div>
</div>
<p>根据两个分布不断迭代就能得到最终两个分布各自的参数，然后两个分布的乘积就是要求的后验分布的近似分布了。</p>

<figure>
    <img src="/images/post/%e3%80%8c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e3%80%8d%e5%8f%98%e5%88%86%e6%8e%a8%e6%96%ad/Variational.jpg"/> 
</figure>


<h2 id="总结">总结</h2>

<p>当我们遇到难搞的后验分布时，可以用容易表达和求解的分布来近似，常规的变分推断基于<strong>平均场理论</strong>，假设近似分布由独立的几部分组成，通过最大化 ELBO 我们可以得到近似分布各个独立部分的迭代求解式，可以理解为积分(求期望)掉 $Z_j$ 外的其他部分得到 $Z_j$ 的更新，类似坐标上升法的思想，先固定其它然后求其中一项。</p>

<p>基于平均场理论的变分推断假设(要求各部分独立)还是比较强的，比如上面的例子中 $\mu$ 和 $\tau$ 其实并不是独立的，$p(\mu/\tau)$ 是个高斯分布；其次这种递推求解的方式中还是涉及了许多的积分，计算量大。因此改进的方法还有<strong>随机梯度变分推断</strong>，感兴趣的可以看看shuhuai大神的讲解。</p>
    </div>
    <div class="read-more">
      <a href="/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/" class="read-more-link">Read more...</a>
    </div>
  </div>
</article>

    </section>
  
  <nav class="pagination">
    
    <a class="next" href="/page/2/">
        <span class="next-text">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
  </nav>
        </div>
        

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mh_chen@zju.edu.cn" class="iconfont icon-email" title="email"></a>
  <a href="https://minghaochen.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2019 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">MinghaoChen</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"  integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>








</body>
</html>
